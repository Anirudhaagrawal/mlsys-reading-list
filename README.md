# LLM Inference Papers

## LLM Serving Systems
Orca - https://usenix.org/system/files/osdi22-yu.pdf

vLLM - https://arxiv.org/abs/2309.06180

Sarathi-serve - https://arxiv.org/abs/2403.02310

## LLM Inference

Sarathi - https://arxiv.org/abs/2308.16369

Splitwise - https://arxiv.org/abs/2311.18677

DistServe - https://arxiv.org/abs/2401.09670

Fairness - https://arxiv.org/abs/2401.00588

APIServe - https://arxiv.org/html/2402.01869v1

SGLang - https://arxiv.org/html/2312.07104v1

LayerSkip - https://arxiv.org/abs/2404.16710

Parrot - https://arxiv.org/abs/2405.19888 (Optional)

FlashAttention - https://arxiv.org/abs/2205.14135

## Multi-Token Prediction 

Lookahead Decoding - https://arxiv.org/abs/2402.02057

Medusa - https://arxiv.org/abs/2401.10774

Speculative Decoding - https://arxiv.org/abs/2211.17192

Multi-Token Prediction - https://arxiv.org/abs/2404.19737


## Multiple LLM and multiple LoRa serving

MuxServe - https://arxiv.org/abs/2404.02015

s-LoRa - https://arxiv.org/abs/2311.03285

## Hallucinations

DoLa - https://arxiv.org/abs/2309.03883

## New Architectures

Mamba - https://arxiv.org/abs/2312.00752

Jamba - https://arxiv.org/abs/2403.19887

## MoE

Gshard - https://usenix.org/system/files/osdi22-yu.pdf

SwapMoE - https://arxiv.org/abs/2308.15030

Towards MoE Deployment - https://arxiv.org/abs/2303.06182

MegaBlocks - https://arxiv.org/abs/2211.15841

Tutel - https://arxiv.org/abs/2206.03382

DeepSpeed-MoE: https://arxiv.org/abs/2201.05596

## Quantization and Compression

GPTQ - https://arxiv.org/abs/2210.17323

SmoothQuant - https://arxiv.org/abs/2211.10438

DynaQuant - https://arxiv.org/abs/2306.11800

Deep Compression - https://arxiv.org/pdf/1510.00149

Linear Quantization - https://arxiv.org/pdf/1712.05877

# Training Papers

Alpa - https://arxiv.org/abs/2201.12023

GPipe - https://arxiv.org/abs/1811.06965

Megatron LM - https://arxiv.org/abs/1909.08053

ZeRO - https://arxiv.org/pdf/1910.02054

# Communication Collectives

Collective Communication review - https://www.cs.utexas.edu/~pingali/CSE392/2011sp/lectures/Conc_Comp.pdf

# Contribution

## Contribute to Our ML Systems Reading List

We’re building a comprehensive and up-to-date GitHub repository of the most impactful papers in Machine Learning Systems (MLSys), and we need your help to keep it growing! Whether you’ve come across a groundbreaking paper, an insightful study, or an innovative idea in the MLSys domain, your contributions can make a difference.

**Why contribute?**

- **Stay Engaged:** Sharing papers not only helps others but also keeps you engaged with the latest research trends and developments in MLSys.
- **Community Growth:** Your contributions foster a collaborative learning environment, helping fellow researchers, engineers, and enthusiasts discover valuable resources.
- **Recognition:** Each contribution will be attributed to you, allowing you to build a visible presence in the MLSys community.

**How to contribute?**

1. **Find a Paper:** Identify a paper that you believe adds value to the repository.
2. **Fork the Repository:** Create a fork of the repository to make your changes.
3. **Add the Paper:** Include the paper in the appropriate section of the reading list, following the existing format.
4. **Open a Pull Request:** Once you’ve added the paper, open a Pull Request (PR) with a brief description of why you think the paper is important.
5. **Engage:** Engage with any feedback on your PR, making revisions as necessary.

By contributing, you’re not just adding a link—you’re helping to shape the learning resources for the next generation of MLSys practitioners. Let’s work together to make this repository the go-to place for anyone interested in the cutting-edge of ML Systems!

---













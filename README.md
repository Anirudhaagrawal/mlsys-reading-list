# LLM Inference Papers

## LLM Serving Systems
Orca - https://usenix.org/system/files/osdi22-yu.pdf

vLLM - https://arxiv.org/abs/2309.06180

Sarathi-serve - https://arxiv.org/abs/2403.02310

Clipper - https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf

InferLine - https://arxiv.org/abs/1812.01776

FlexGen - https://arxiv.org/abs/2303.06865

## LLM Inference

Sarathi - https://arxiv.org/abs/2308.16369

Splitwise - https://arxiv.org/abs/2311.18677

DistServe - https://arxiv.org/abs/2401.09670

Fairness - https://arxiv.org/abs/2401.00588

APIServe - https://arxiv.org/html/2402.01869v1

SGLang - https://arxiv.org/html/2312.07104v1

LayerSkip - https://arxiv.org/abs/2404.16710

Parrot - https://arxiv.org/abs/2405.19888

Flash Decode - https://arxiv.org/abs/2311.01282

## Multi-Token Prediction 

Lookahead Decoding - https://arxiv.org/abs/2402.02057

Medusa - https://arxiv.org/abs/2401.10774

Speculative Decoding - https://arxiv.org/abs/2211.17192

Multi-Token Prediction - https://arxiv.org/abs/2404.19737


## Multiple LLM and multiple LoRa serving

MuxServe - https://arxiv.org/abs/2404.02015

s-LoRa - https://arxiv.org/abs/2311.03285

## Hallucinations

DoLa - https://arxiv.org/abs/2309.03883

## New Architectures

Mamba - https://arxiv.org/abs/2312.00752

Jamba - https://arxiv.org/abs/2403.19887

## MoE

Gshard - https://usenix.org/system/files/osdi22-yu.pdf

SwapMoE - https://arxiv.org/abs/2308.15030

Towards MoE Deployment - https://arxiv.org/abs/2303.06182

MegaBlocks - https://arxiv.org/abs/2211.15841

Tutel - https://arxiv.org/abs/2206.03382

DeepSpeed-MoE: https://arxiv.org/abs/2201.05596

## Hardware-Aware Algorithm Design

FlashAttention - https://arxiv.org/abs/2205.14135

FlashAttention 2 - https://arxiv.org/abs/2307.08691

FlashAttention 3 - https://tridao.me/publications/flash3/flash3.pdf

Multi Query Attention - https://arxiv.org/abs/1911.02150v1

Grouped Query Attention - https://arxiv.org/abs/2305.13245v3

## Training Papers

Alpa - https://arxiv.org/abs/2201.12023

GPipe - https://arxiv.org/abs/1811.06965

Megatron LM - https://arxiv.org/abs/1909.08053

ZeRO - https://arxiv.org/pdf/1910.02054

Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM - https://arxiv.org/abs/2104.04473

Varuna - https://arxiv.org/abs/2111.04007

Llama 3.1 - https://ai.meta.com/research/publications/the-llama-3-herd-of-models/

PyTorch Distributed - https://arxiv.org/abs/2006.15704

LLM Training Puzzles - https://github.com/srush/LLM-Training-Puzzles?tab=readme-ov-file

# Tiny ML

## Quantization and Compression

GPTQ - https://arxiv.org/abs/2210.17323

SmoothQuant - https://arxiv.org/abs/2211.10438

DynaQuant - https://arxiv.org/abs/2306.11800

Deep Compression - https://arxiv.org/pdf/1510.00149

Linear Quantization - https://arxiv.org/pdf/1712.05877


# Communication Collectives

Collective Communication review - https://www.cs.utexas.edu/~pingali/CSE392/2011sp/lectures/Conc_Comp.pdf

MSCCLang - https://parsa.epfl.ch/course-info/cs723/papers/MSCCLang.pdf

TACCL - https://www.usenix.org/system/files/nsdi23-shah.pdf

An In-Network Architecture for Accelerating
Shared-Memory Multiprocessor Collectives - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9138924

Synthesizing Optimal Collective Algorithms  - https://dl.acm.org/doi/pdf/10.1145/3437801.3441620


Breaking the Computation and Communication Abstraction
Barrier in Distributed Machine Learning Workloads - https://arxiv.org/pdf/2105.05720



# Contribution

## Contribute to Our ML Systems Reading List

We’re building a comprehensive and up-to-date GitHub repository of the most impactful papers in Machine Learning Systems (MLSys), and we need your help to keep it growing! Whether you’ve come across a groundbreaking paper, an insightful study, or an innovative idea in the MLSys domain, your contributions can make a difference.

**Why contribute?**

- **Stay Engaged:** Sharing papers not only helps others but also keeps you engaged with the latest research trends and developments in MLSys.
- **Community Growth:** Your contributions foster a collaborative learning environment, helping fellow researchers, engineers, and enthusiasts discover valuable resources.
- **Recognition:** Each contribution will be attributed to you, allowing you to build a visible presence in the MLSys community.

**How to contribute?**

1. **Find a Paper:** Identify a paper that you believe adds value to the repository.
2. **Fork the Repository:** Create a fork of the repository to make your changes.
3. **Add the Paper:** Include the paper in the appropriate section of the reading list, following the existing format.
4. **Open a Pull Request:** Once you’ve added the paper, open a Pull Request (PR) with a brief description of why you think the paper is important.
5. **Engage:** Engage with any feedback on your PR, making revisions as necessary.

By contributing, you’re not just adding a link—you’re helping to shape the learning resources for the next generation of MLSys practitioners. Let’s work together to make this repository the go-to place for anyone interested in the cutting-edge of ML Systems!

---












